# OpenAI-compatible Chat client (concise reference)

This package provides a compact, asyncio-based OpenAI-compatible chat client with three core features:

- Text chat (non-streaming and streaming)
- Multimodal inputs (image and video)
- Tool/function calling with optional automatic execution

Quick usage (minimal):

1) Install dependency:

```powershell
pip install httpx
```

2) Basic text chat example:

```python
import asyncio
from openai_chatapi import ChatAgent, ModelConfig, RuntimeConfig

async def main():
    cfg = ModelConfig(api_base_url="https://your.api", api_key="sk-...", model="gpt-3.5-turbo")
    rt = RuntimeConfig(enable_logging=True)
    async with ChatAgent(cfg, rt) as agent:
        resp = await agent.chat("Hello")
        print(resp)

asyncio.run(main())
```

3) Streaming example:

```python
# use `agent.chat_stream(...)` with `async for chunk in agent.chat_stream(...)` to receive chunks
```

4) Multimodal example:

```python
# pass image paths: image_paths="photo.jpg" or image_paths=["a.jpg","b.jpg"]
# pass video path: video_paths="video.mp4"
```

Examples (in `openai_chatapi/examples`):
- `example_1_basic_chat.py` â€” basic text chat
- `example_3_streaming.py` â€” streaming usage
- `example_4_multimodal.py` â€” image/video examples

That is all â€” this README contains only concise usage instructions and links to a few examples.
) -> str
```

**chat_stream()** - æµå¼å¯¹è¯
```python
async def chat_stream(
    text: str,
    image_paths: Union[str, List[str], None] = None,
    video_paths: Union[str, List[str], None] = None,
    add_to_history: bool = True,
    display_stream: bool = True,
    **kwargs
) -> AsyncGenerator[str, None]
```

**å·¥å…·ç®¡ç†**
```python
register_tool(tool: Tool, handler: Callable) -> None
clear_tools() -> None
```

**æ¶ˆæ¯ç®¡ç†**
```python
set_system_prompt(prompt: str) -> None
add_message(message: ChatMessage) -> None
clear_history(keep_system: bool = True) -> None
```

**ç»Ÿè®¡ä¿¡æ¯**
```python
get_stats() -> dict
reset_stats() -> None
```

---

### ModelConfig

æ¨¡å‹è¡Œä¸ºé…ç½®ã€‚

```python
@dataclass
class ModelConfig:
    # APIè¿æ¥
    api_base_url: str = "https://api.openai.com/v1"
    api_key: Optional[str] = None
    model: str = "gpt-4o"
    
    # é‡‡æ ·å‚æ•°
    temperature: float = 0.7
    top_p: float = 1.0
    
    # é•¿åº¦æ§åˆ¶
    max_tokens: Optional[int] = None
    max_completion_tokens: Optional[int] = None
    
    # æƒ©ç½š
    frequency_penalty: float = 0.0
    presence_penalty: float = 0.0
    
    # å…¶ä»–
    n: int = 1
    stop: Optional[Union[str, List[str]]] = None
    stream: bool = False
    logprobs: Optional[bool] = None
    top_logprobs: Optional[int] = None
    seed: Optional[int] = None
    user: Optional[str] = None
    reasoning_effort: Optional[str] = None  # "low", "medium", "high"
```

---

### RuntimeConfig

è¿è¡Œæ—¶è¡Œä¸ºé…ç½®ã€‚

```python
@dataclass
class RuntimeConfig:
    # æ—¥å¿—
    enable_logging: bool = True
    log_level: str = "INFO"
    log_http_requests: bool = False
    log_http_responses: bool = False
    
    # ç›‘æ§
    enable_debug: bool = False
    capture_token_usage: bool = True
    capture_latency: bool = True
    
    # HTTP
    timeout: int = 60
    verify_ssl: bool = True
    max_retries: int = 0
    retry_delay: float = 1.0
    
    # æµå¼
    stream_chunk_callback: Optional[Callable[[str], None]] = None
    stream_enable_progress: bool = False
    
    # è§£æ
    strict_parsing: bool = False
    truncate_long_errors: bool = True
    max_error_length: int = 500
```

---

### å¼‚å¸¸ç±»å‹

æ‰€æœ‰å¼‚å¸¸ç»§æ‰¿è‡ª `ChatAPIException`ï¼š

- **ConfigurationError** - é…ç½®éªŒè¯é”™è¯¯
- **APIConnectionError** - API è¿æ¥é”™è¯¯
- **APIResponseError** - API å“åº”è§£æé”™è¯¯
- **ToolExecutionError** - å·¥å…·æ‰§è¡Œé”™è¯¯
- **MediaProcessingError** - åª’ä½“å¤„ç†é”™è¯¯
- **ModelNotFoundError** - æ¨¡å‹æœªæ‰¾åˆ°
- **TokenLimitError** - Token é™åˆ¶è¶…å‡º

æ¯ä¸ªå¼‚å¸¸éƒ½åŒ…å« `message` å’Œ `details` å­—å…¸ã€‚

---

## ğŸ¯ ä½¿ç”¨åœºæ™¯

### åœºæ™¯1ï¼šç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

```python
# å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œç›‘æ§
runtime_config = RuntimeConfig(
    enable_logging=True,
    log_level="WARNING",
    capture_token_usage=True,
    capture_latency=True,
    max_retries=3,
    timeout=30,
)

try:
    async with ChatAgent(model_config, runtime_config) as agent:
        response = await agent.chat(user_input)
        
        # è®°å½•ç»Ÿè®¡
        stats = agent.get_stats()
        logging.info(f"Request completed, tokens: {stats['total_tokens']}")
        
except ChatAPIException as e:
    logging.error(f"Chat failed: {e}")
    # é”™è¯¯æ¢å¤é€»è¾‘
```

### åœºæ™¯2ï¼šå¼€å‘è°ƒè¯•

```python
# è¯¦ç»†æ—¥å¿—å’Œè°ƒè¯•ä¿¡æ¯
runtime_config = RuntimeConfig(
    enable_logging=True,
    log_level="DEBUG",
    log_http_requests=True,
    log_http_responses=True,
    enable_debug=True,
    verify_ssl=False,  # æœ¬åœ°æµ‹è¯•
)

async with ChatAgent(model_config, runtime_config) as agent:
    # æ‰€æœ‰è¯·æ±‚/å“åº”éƒ½ä¼šè¢«è¯¦ç»†è®°å½•
    response = await agent.chat("test")
```

### åœºæ™¯3ï¼šæµå¼å‰ç«¯å±•ç¤º

```python
from fastapi import FastAPI
from fastapi.responses import StreamingResponse

app = FastAPI()

@app.get("/chat")
async def chat_endpoint(text: str):
    async def generate():
        async with ChatAgent(model_config) as agent:
            async for chunk in agent.chat_stream(text, display_stream=False):
                yield f"data: {chunk}\n\n"
    
    return StreamingResponse(generate(), media_type="text/event-stream")
```

---

## ğŸ“ æ›´æ–°æ—¥å¿—

### v0.3.0 (å½“å‰ç‰ˆæœ¬)

**æ–°åŠŸèƒ½:**
- âœ… è§†é¢‘è¾“å…¥æ”¯æŒ
- âœ… åˆ†ç¦»çš„é…ç½®ç³»ç»Ÿï¼ˆModelConfig + RuntimeConfigï¼‰
- âœ… å®Œå–„çš„å¼‚å¸¸å¤„ç†ä½“ç³»ï¼ˆ8ç§å¼‚å¸¸ç±»å‹ï¼‰
- âœ… Tokenä½¿ç”¨ç»Ÿè®¡å’Œå»¶è¿Ÿè¿½è¸ª
- âœ… æµå¼å“åº”å®æ—¶æ˜¾ç¤º
- âœ… HTTP è¯·æ±‚/å“åº”æ—¥å¿—
- âœ… è°ƒè¯•æ¨¡å¼

**æ”¹è¿›:**
- âœ… æ›´å¥½çš„é”™è¯¯è¯Šæ–­ä¿¡æ¯
- âœ… è‡ªåŠ¨æˆªæ–­é•¿é”™è¯¯æ¶ˆæ¯
- âœ… å‘åå…¼å®¹æ—§ APIï¼ˆChatConfigï¼‰

**ç ´åæ€§å˜æ›´:**
- `ChatConfig` æ”¹åä¸º `ModelConfig`ï¼ˆä¿ç•™åˆ«åå…¼å®¹ï¼‰
- `ChatAgent.__init__` ç°åœ¨æ¥å—ä¸¤ä¸ªé…ç½®å‚æ•°

### v0.2.0

- å·¥å…·è°ƒç”¨æ”¯æŒ
- æ¨¡å‹ç®¡ç†æ¨¡å—
- å®Œæ•´çš„ OpenAI å‚æ•°æ”¯æŒ

### v0.1.0

- åŸºç¡€æ–‡æœ¬å¯¹è¯
- å›¾åƒè¾“å…¥æ”¯æŒ
- æµå¼å“åº”

---

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## ğŸ“„ è®¸å¯

MIT License

---

## ğŸ”— ç›¸å…³é“¾æ¥

- [OpenAI API æ–‡æ¡£](https://platform.openai.com/docs/api-reference)
- [httpx æ–‡æ¡£](https://www.python-httpx.org/)

---

## â“ å¸¸è§é—®é¢˜

**Q: å¦‚ä½•ä½¿ç”¨æœ¬åœ°æ¨¡å‹æœåŠ¡ï¼ˆå¦‚ Ollamaï¼‰ï¼Ÿ**

A: åªéœ€ä¿®æ”¹ `api_base_url`ï¼š
```python
model_config = ModelConfig(
    api_base_url="http://localhost:11434/v1",
    api_key="not-needed",  # Ollama ä¸éœ€è¦ key
    model="llama2",
)
```

**Q: å¦‚ä½•å¤„ç†è‡ªç­¾å SSL è¯ä¹¦ï¼Ÿ**

A: åœ¨ RuntimeConfig ä¸­å…³é—­éªŒè¯ï¼š
```python
runtime_config = RuntimeConfig(verify_ssl=False)
```

**Q: æµå¼è¾“å‡ºä¸æ˜¾ç¤ºæ€ä¹ˆåŠï¼Ÿ**

A: ç¡®ä¿å¯ç”¨äº†è¿›åº¦æ˜¾ç¤ºï¼š
```python
runtime_config = RuntimeConfig(stream_enable_progress=True)
async for chunk in agent.chat_stream(text, display_stream=True):
    pass
```

**Q: å¦‚ä½•è¿½è¸ª Token ä½¿ç”¨é‡ï¼Ÿ**

A: å¯ç”¨ç»Ÿè®¡è¿½è¸ªï¼š
```python
runtime_config = RuntimeConfig(capture_token_usage=True)
# ... ä½¿ç”¨ agent ...
stats = agent.get_stats()
print(stats)
```

**Q: æ”¯æŒå“ªäº›è§†é¢‘æ ¼å¼ï¼Ÿ**

A: æ”¯æŒå¸¸è§æ ¼å¼ï¼šmp4, webm, ogg, mov, aviã€‚è§†é¢‘ä¼šè¢« Base64 ç¼–ç åå‘é€ï¼Œæ³¨æ„å¤§å°é™åˆ¶ã€‚

**Q: å·¥å…·è°ƒç”¨å¤±è´¥æ€ä¹ˆåŠï¼Ÿ**

A: æ•è· `ToolExecutionError` æŸ¥çœ‹è¯¦æƒ…ï¼š
```python
try:
    response = await agent.chat("...", auto_execute_tools=True)
except ToolExecutionError as e:
    print(f"Tool failed: {e.details['tool']}")
    print(f"Error: {e.details['error']}")
```
