"""
ARCHIVED: examples_v0.3.py

This file contained a large set of example scenarios for older
versions. It has been archived to keep the examples directory concise.
Active examples moved to:
- example_1_basic_chat.py
- example_3_streaming.py
- example_4_multimodal.py

Do not edit this file; it exists only to keep historical context.
"""
print("examples_v0.3.py has been archived. Use the concise examples instead.")
        
        except APIConnectionError as e:
            print("✓ 成功捕获 APIConnectionError")
            print(f"  错误信息: {e.message}")
            print(f"  错误详情:")
            for key, value in e.details.items():
                print(f"    - {key}: {value}")


async def example_6_backward_compatibility():
    """示例6: 向后兼容（旧API）"""
    print("\n" + "="*60)
    print("示例6: 向后兼容（使用旧的ChatConfig）")
    print("="*60)
    
    # 旧的API仍然可用
    from openai_chatapi import ChatConfig
    
    config = ChatConfig(  # ChatConfig 是 ModelConfig 的别名
        api_base_url="https://api.openai.com/v1",
        api_key="your-key",
        model="gpt-4o",
    )
    
    # 旧的单参数初始化仍然支持
    async with ChatAgent(config) as agent:
        print("✓ 向后兼容成功")
        print("  - ChatConfig 仍可使用（是 ModelConfig 的别名）")
        print("  - 单参数初始化仍然支持")
        print("  - 所有旧代码无需修改")


async def example_7_custom_callback():
    """示例7: 自定义流式回调"""
    print("\n" + "="*60)
    print("示例7: 自定义流式回调函数")
    print("="*60)
    
    chunks_received = []
    
    def my_callback(chunk: str):
        """自定义回调处理每个chunk"""
        chunks_received.append(chunk)
        # 可以在这里发送到前端、写入文件等
        sys.stdout.write(f"[Chunk {len(chunks_received)}] ")
        sys.stdout.flush()
    
    model_config = ModelConfig(
        api_base_url="http://localhost:11434/v1",
        api_key="not-needed",
        model="qwen2.5:7b",
    )
    
    runtime_config = RuntimeConfig(
        stream_chunk_callback=my_callback,  # 设置回调
        verify_ssl=False,
    )
    
    async with ChatAgent(model_config, runtime_config) as agent:
        try:
            async for chunk in agent.chat_stream("说句话", display_stream=False):
                pass  # 回调会自动处理
            
            print(f"\n✓ 通过回调接收了 {len(chunks_received)} 个chunks")
            
        except APIConnectionError:
            print("\n✗ 连接错误（需要API服务）")


async def example_8_debug_mode():
    """示例8: 调试模式"""
    print("\n" + "="*60)
    print("示例8: 调试模式（详细日志）")
    print("="*60)
    
    model_config = ModelConfig(
        api_base_url="http://localhost:11434/v1",
        model="qwen2.5:7b",
    )
    
    runtime_config = RuntimeConfig(
        enable_debug=True,
        log_level="DEBUG",
        log_http_requests=True,   # 记录HTTP请求
        log_http_responses=False,  # 不记录响应（太长）
        verify_ssl=False,
    )
    
    async with ChatAgent(model_config, runtime_config) as agent:
        print("✓ 调试模式已启用")
        print("  所有操作都会产生详细日志")
        print("  （需要实际API调用才能看到HTTP日志）")


async def main():
    """运行所有示例"""
    print("="*60)
    print("OpenAI Chat API Client v0.3.0 - 功能演示")
    print("="*60)
    
    examples = [
        ("新配置系统", example_1_new_config),
        ("视频输入", example_2_video_input),
        ("流式显示", example_3_streaming_display),
        ("统计追踪", example_4_statistics),
        ("错误处理", example_5_error_handling),
        ("向后兼容", example_6_backward_compatibility),
        ("自定义回调", example_7_custom_callback),
        ("调试模式", example_8_debug_mode),
    ]
    
    for name, func in examples:
        try:
            await func()
        except Exception as e:
            print(f"\n✗ 示例 '{name}' 执行出错: {e}")
    
    print("\n" + "="*60)
    print("演示完成！")
    print("="*60)


if __name__ == "__main__":
    asyncio.run(main())
