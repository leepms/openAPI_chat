#!/usr/bin/env python3
"""
ç¤ºä¾‹ 3: æµå¼è¾“å‡º
æ¼”ç¤ºå¦‚ä½•å®æ—¶æ˜¾ç¤º AI å“åº”ï¼ˆæ‰“å­—æœºæ•ˆæœï¼‰
"""

# å¯ç¼–è¾‘ï¼šä¼˜å…ˆåœ¨è¿™é‡Œå¡«å†™ API å‚æ•°ï¼Œç•™ç©º (None) åˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶
API_KEY = "sk-0253dd96205d4d83b0b792e08dfaec06"  # e.g. "sk-..." æˆ– None
API_BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"  # e.g. "https://api.openai.com/v1" æˆ– None
MODEL = "qwen3-32b"

import asyncio
import os
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from openai_chatapi import ChatAgent, ModelConfig, RuntimeConfig

# å¡«å…… API å‚æ•°
if API_KEY is None:
    API_KEY = os.getenv("OPENAI_API_KEY")
if API_BASE_URL is None:
    API_BASE_URL = os.getenv("OPENAI_API_BASE_URL")

if API_KEY is None or API_BASE_URL is None:
    try:
        cfg_path = Path(__file__).parent.parent / "config" / "default_model_config.yaml"
        if cfg_path.exists():
            cfg = ModelConfig.from_yaml(str(cfg_path))
            if API_KEY is None:
                API_KEY = cfg.api_key
            if API_BASE_URL is None:
                API_BASE_URL = cfg.api_base_url
    except Exception:
        pass


async def main():
    print("=" * 60)
    print("ç¤ºä¾‹ 3: æµå¼è¾“å‡º")
    print("=" * 60)
    print()

    # é…ç½®ï¼ˆä½¿ç”¨é¡¶éƒ¨å˜é‡æˆ–ç¯å¢ƒ/é…ç½®æ–‡ä»¶ï¼‰
    model_config = ModelConfig(
        api_key=API_KEY,
        api_base_url=API_BASE_URL,
        model=MODEL,
    )

    runtime_config = RuntimeConfig(
        enable_logging=True,
        stream_enable_progress=True,
    )

    async with ChatAgent(model_config, runtime_config) as agent:

        # ç¤ºä¾‹ 1: è‡ªåŠ¨æ˜¾ç¤ºæµå¼è¾“å‡º
        print("ã€ç¤ºä¾‹ 1: è‡ªåŠ¨æ˜¾ç¤ºã€‘")
        print("ğŸ’¬ ç”¨æˆ·: è¯·ç”¨ä¸‰å¥è¯ä»‹ç»äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹ã€‚\n")
        print("ğŸ¤– åŠ©æ‰‹: ", end="", flush=True)

        async for _chunk in agent.chat_stream(
            "è¯·ç”¨ä¸‰å¥è¯ä»‹ç»äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹ã€‚",
            display_stream=True  # è‡ªåŠ¨æ‰“å°æ¯ä¸ª chunk
        ):
            pass  # chunk å·²è‡ªåŠ¨æ‰“å°

        print("\n" + "-" * 60 + "\n")

        # ç¤ºä¾‹ 2: æ‰‹åŠ¨å¤„ç†æ¯ä¸ª chunk
        print("ã€ç¤ºä¾‹ 2: æ‰‹åŠ¨å¤„ç†ã€‘")
        print("ğŸ’¬ ç”¨æˆ·: ç”¨ä¸€å¥è¯æ€»ç»“æœºå™¨å­¦ä¹ ã€‚\n")
        print("ğŸ¤– åŠ©æ‰‹: ", end="", flush=True)

        chunks = []
        async for chunk in agent.chat_stream(
            "ç”¨ä¸€å¥è¯æ€»ç»“æœºå™¨å­¦ä¹ ã€‚",
            display_stream=False  # ä¸è‡ªåŠ¨æ‰“å°
        ):
            # æ‰‹åŠ¨å¤„ç†æ¯ä¸ª chunkï¼ˆå¯ä»¥æ·»åŠ è‡ªå®šä¹‰é€»è¾‘ï¼‰
            print(chunk, end="", flush=True)
            chunks.append(chunk)

        full_response = "".join(chunks)
        print("\n")
        print(f"\nğŸ“ å®Œæ•´å“åº”é•¿åº¦: {len(full_response)} å­—ç¬¦")
        print(f"ğŸ“¦ æµå¼ chunks: {len(chunks)} ä¸ª")

        # ç»Ÿè®¡ä¿¡æ¯
        stats = agent.get_stats()
        print("\n" + "=" * 60)
        print(f"è¯·æ±‚æ¬¡æ•°: {stats['total_requests']} | "
              f"å¹³å‡å»¶è¿Ÿ: {stats.get('average_latency', 0.0):.2f}s")
        print("=" * 60)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nå·²å–æ¶ˆ")
    except Exception as e:
        print(f"\né”™è¯¯: {e}")
