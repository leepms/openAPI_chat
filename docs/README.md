# OpenAI Compatible Chat API Client

ä¸€ä¸ªåŠŸèƒ½å®Œæ•´çš„ OpenAI å…¼å®¹æ ¼å¼èŠå¤© API å®¢æˆ·ç«¯ï¼Œæ”¯æŒæ‰€æœ‰ä¸»æµ OpenAI API ç‰¹æ€§ã€‚

## âœ¨ æ ¸å¿ƒç‰¹æ€§

### ğŸ¯ å¤šæ¨¡æ€æ”¯æŒ
- âœ… æ–‡æœ¬ã€å›¾åƒã€**è§†é¢‘**è¾“å…¥ï¼ˆå•ä¸ªæˆ–å¤šä¸ªï¼‰
- âœ… çµæ´»çš„å†…å®¹ç»„åˆæ–¹å¼
- âœ… è‡ªåŠ¨ Base64 ç¼–ç å¤„ç†

### ğŸ”„ å“åº”æ¨¡å¼
- âœ… éæµå¼å“åº” - å®Œæ•´å“åº”ä¸€æ¬¡è¿”å›
- âœ… æµå¼å“åº” - **å®æ—¶æ˜¾ç¤º**ï¼Œæ”¯æŒè¿›åº¦å›è°ƒ
- âœ… å·¥å…·è°ƒç”¨ - è‡ªåŠ¨æ‰§è¡Œå’Œè¿­ä»£æ§åˆ¶

### ğŸ› ï¸ å·¥å…·è°ƒç”¨ (Function Calling)
- âœ… å®Œæ•´çš„ OpenAI Tool/Function Calling æ”¯æŒ
- âœ… åŒæ­¥å’Œå¼‚æ­¥å·¥å…·å¤„ç†å‡½æ•°
- âœ… è‡ªåŠ¨å·¥å…·æ‰§è¡Œå’Œç»“æœå¤„ç†
- âœ… å¤šè½®å·¥å…·è°ƒç”¨æ§åˆ¶

### ğŸ“Š æ¨¡å‹ç®¡ç†
- âœ… è‡ªåŠ¨è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
- âœ… æ™ºèƒ½æ¨¡å‹é€‰æ‹©
- âœ… æ¨¡å‹è¯¦ç»†ä¿¡æ¯æŸ¥è¯¢

### âš™ï¸ å®Œæ•´çš„ OpenAI API å‚æ•°
- âœ… é‡‡æ ·æ§åˆ¶ - `temperature`, `top_p`
- âœ… é•¿åº¦æ§åˆ¶ - `max_tokens`, `max_completion_tokens`
- âœ… æƒ©ç½šå‚æ•° - `frequency_penalty`, `presence_penalty`
- âœ… é«˜çº§ç‰¹æ€§ - `logprobs`, `seed`, `stop`, `n`
- âœ… æ¨ç†æ¨¡å‹ - `reasoning_effort` (o1 ç³»åˆ—)

### ğŸ”§ è¿è¡Œæ—¶é…ç½®
- âœ… **åˆ†ç¦»çš„é…ç½®ç³»ç»Ÿ** - æ¨¡å‹é…ç½® + è¿è¡Œæ—¶é…ç½®
- âœ… æ—¥å¿—æ§åˆ¶ - æ—¥å¿—çº§åˆ«ã€HTTP æ—¥å¿—
- âœ… **ç»Ÿè®¡è¿½è¸ª** - Token ä½¿ç”¨é‡ã€å»¶è¿Ÿã€æˆåŠŸç‡
- âœ… è°ƒè¯•æ¨¡å¼ - è¯¦ç»†çš„è°ƒè¯•è¾“å‡º
- âœ… SSL æ§åˆ¶ - æ”¯æŒè‡ªç­¾åè¯ä¹¦

### ğŸ›¡ï¸ é”™è¯¯å¤„ç†
- âœ… **å®Œå–„çš„å¼‚å¸¸ä½“ç³»** - 8 ç§ä¸“ç”¨å¼‚å¸¸ç±»å‹
- âœ… è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œè¯Šæ–­
- âœ… è‡ªåŠ¨é”™è¯¯è·Ÿè¸ªå’Œç»Ÿè®¡

### ğŸ’¡ æŠ€æœ¯ç‰¹ç‚¹
- âœ… **æœ€å°ä¾èµ–** - ä»…éœ€ httpx
- âœ… ç±»å‹å®‰å…¨ - ä½¿ç”¨ Python dataclass
- âœ… å®Œå…¨å¼‚æ­¥ - åŸºäº async/await
- âœ… å‘åå…¼å®¹ - ä¿æŒæ—§ API å…¼å®¹

---

## ğŸ“¦ å®‰è£…

```bash
pip install httpx
```

æˆ–ä½¿ç”¨ requirements.txt:

```bash
cd openai_chatapi
pip install -r requirements.txt
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åŸºæœ¬æ–‡æœ¬å¯¹è¯

```python
import asyncio
from openai_chatapi import ChatAgent, ModelConfig, RuntimeConfig

async def main():
    # æ¨¡å‹é…ç½®
    model_config = ModelConfig(
        api_base_url="https://api.openai.com/v1",
        api_key="your-api-key",
        model="gpt-4o",
        temperature=0.7,
    )
    
    # è¿è¡Œæ—¶é…ç½®ï¼ˆå¯é€‰ï¼‰
    runtime_config = RuntimeConfig(
        enable_logging=True,
        log_level="INFO",
        capture_token_usage=True,
    )
    
    async with ChatAgent(model_config, runtime_config) as agent:
        agent.set_system_prompt("You are a helpful assistant.")
        
        response = await agent.chat("Hello, how are you?")
        print(response)
        
        # æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
        print(agent.get_stats())

asyncio.run(main())
```

**å‘åå…¼å®¹å†™æ³•**ï¼ˆä½¿ç”¨ ChatConfigï¼‰:
```python
from openai_chatapi import ChatAgent, ChatConfig

config = ChatConfig(api_base_url="...", api_key="...", model="gpt-4o")
async with ChatAgent(config) as agent:
    response = await agent.chat("Hello!")
```

---

### 2. å¤šæ¨¡æ€è¾“å…¥ï¼ˆå›¾åƒ+è§†é¢‘ï¼‰

```python
# å›¾åƒè¾“å…¥
response = await agent.chat(
    "What's in this image?",
    image_paths="photo.jpg"
)

# å¤šå¼ å›¾åƒ
response = await agent.chat(
    "Compare these images",
    image_paths=["image1.jpg", "image2.jpg"]
)

# è§†é¢‘è¾“å…¥ï¼ˆæ–°åŠŸèƒ½ï¼‰
response = await agent.chat(
    "Describe what happens in this video",
    video_paths="video.mp4"
)

# æ··åˆè¾“å…¥
response = await agent.chat(
    "Analyze these media files",
    image_paths=["photo1.jpg", "photo2.png"],
    video_paths="demo.mp4"
)
```

---

### 3. æµå¼å“åº”ï¼ˆå®æ—¶æ˜¾ç¤ºï¼‰

```python
# å¯ç”¨å®æ—¶æ˜¾ç¤º
runtime_config = RuntimeConfig(
    stream_enable_progress=True,  # å®æ—¶æ‰“å°
)

async with ChatAgent(model_config, runtime_config) as agent:
    # æµå¼è¾“å‡ºä¼šè‡ªåŠ¨åœ¨ç»ˆç«¯æ˜¾ç¤º
    async for chunk in agent.chat_stream(
        "Tell me a long story", 
        display_stream=True
    ):
        # chunk ä¼šè‡ªåŠ¨æ‰“å°ï¼Œä¹Ÿå¯ä»¥æ‰‹åŠ¨å¤„ç†
        pass

# è‡ªå®šä¹‰å›è°ƒå¤„ç†
def my_callback(chunk: str):
    # å¤„ç†æ¯ä¸ªchunkï¼Œä¾‹å¦‚å‘é€åˆ°å‰ç«¯
    print(f"Received: {chunk}")

runtime_config = RuntimeConfig(
    stream_chunk_callback=my_callback
)
```

---

### 4. å·¥å…·è°ƒç”¨ï¼ˆFunction Callingï¼‰

```python
from openai_chatapi import Tool, FunctionDefinition

# å®šä¹‰å·¥å…·å‡½æ•°
def get_weather(location: str) -> dict:
    """è·å–å¤©æ°”ä¿¡æ¯"""
    return {"temperature": 22, "condition": "sunny", "location": location}

# å®šä¹‰å·¥å…·æè¿°
weather_tool = Tool(
    type="function",
    function=FunctionDefinition(
        name="get_weather",
        description="Get weather information for a location",
        parameters={
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "City name"
                }
            },
            "required": ["location"]
        }
    )
)

# æ³¨å†Œå·¥å…·
agent.register_tool(weather_tool, get_weather)

# è‡ªåŠ¨æ‰§è¡Œå·¥å…·è°ƒç”¨
response = await agent.chat(
    "What's the weather in Paris?",
    auto_execute_tools=True,  # è‡ªåŠ¨æ‰§è¡Œ
    max_tool_iterations=5      # æœ€å¤š5è½®
)
print(response)  # "The weather in Paris is sunny with 22Â°C."
```

---

### 5. é«˜çº§é…ç½®

```python
# å®Œæ•´çš„æ¨¡å‹é…ç½®
model_config = ModelConfig(
    api_base_url="https://api.openai.com/v1",
    api_key="sk-...",
    model="gpt-4o",
    
    # é‡‡æ ·å‚æ•°
    temperature=0.8,
    top_p=0.9,
    
    # é•¿åº¦æ§åˆ¶
    max_tokens=2000,
    
    # æƒ©ç½šå‚æ•°
    frequency_penalty=0.5,
    presence_penalty=0.5,
    
    # é«˜çº§ç‰¹æ€§
    seed=42,              # å¯å¤ç°æ€§
    logprobs=True,        # è¿”å›æ¦‚ç‡
    top_logprobs=3,       # Top-3 æ¦‚ç‡
    stop=["END"],         # åœæ­¢åºåˆ—
    n=1,                  # ç”Ÿæˆæ•°é‡
)

# å®Œæ•´çš„è¿è¡Œæ—¶é…ç½®
runtime_config = RuntimeConfig(
    # æ—¥å¿—é…ç½®
    enable_logging=True,
    log_level="DEBUG",
    log_http_requests=True,   # è®°å½•HTTPè¯·æ±‚
    log_http_responses=True,  # è®°å½•HTTPå“åº”
    
    # ç›‘æ§å’Œè°ƒè¯•
    enable_debug=True,
    capture_token_usage=True,  # è¿½è¸ªtokenä½¿ç”¨
    capture_latency=True,      # è¿½è¸ªå»¶è¿Ÿ
    
    # HTTPè¡Œä¸º
    timeout=120,
    verify_ssl=False,          # æ”¯æŒè‡ªç­¾åè¯ä¹¦
    max_retries=3,             # é‡è¯•æ¬¡æ•°
    retry_delay=2.0,           # é‡è¯•å»¶è¿Ÿ
    
    # æµå¼é…ç½®
    stream_enable_progress=True,        # å®æ—¶æ˜¾ç¤º
    stream_chunk_callback=my_callback,  # è‡ªå®šä¹‰å›è°ƒ
    
    # é”™è¯¯å¤„ç†
    strict_parsing=False,      # å®½æ¾è§£æ
    truncate_long_errors=True, # æˆªæ–­é•¿é”™è¯¯
)

async with ChatAgent(model_config, runtime_config) as agent:
    # ä½¿ç”¨é…ç½®
    response = await agent.chat("Hello")
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = agent.get_stats()
    print(f"Total requests: {stats['total_requests']}")
    print(f"Total tokens: {stats['total_tokens']}")
    print(f"Average latency: {stats['average_latency']}s")
    print(f"Success rate: {stats['success_rate']}%")
```

---

### 6. æ¨¡å‹ç®¡ç†

```python
from openai_chatapi import ModelManager

manager = ModelManager(
    api_base_url="https://api.openai.com/v1",
    api_key="your-key",
    verify_ssl=False  # å¦‚éœ€è¦
)

# è·å–æ‰€æœ‰æ¨¡å‹
models = await manager.list_models()
print(f"Available models: {models}")

# è·å–è¯¦ç»†ä¿¡æ¯
detailed = await manager.list_models_detailed()
for model in detailed:
    print(f"{model.id}: owned by {model.owned_by}")

# è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹
selected = await manager.select_model(["gpt-4o", "gpt-4-turbo", "gpt-3.5-turbo"])
print(f"Selected: {selected}")

# è·å–ç‰¹å®šæ¨¡å‹ä¿¡æ¯
info = await manager.get_model_info("gpt-4o")
print(info.to_dict())
```

---

### 7. æ¨ç†æ¨¡å‹ (o1 ç³»åˆ—)

```python
# o1 ç³»åˆ—æ¨¡å‹ç‰¹æ®Šé…ç½®
model_config = ModelConfig(
    model="o1-preview",
    reasoning_effort="high",  # "low", "medium", "high"
    max_completion_tokens=5000,  # o1 ä½¿ç”¨ max_completion_tokens
)

async with ChatAgent(model_config) as agent:
    response = await agent.chat("Solve this complex math problem: ...")
    print(response)
```

---

### 8. é”™è¯¯å¤„ç†

```python
from openai_chatapi import (
    ChatAPIException,
    APIConnectionError,
    APIResponseError,
    ToolExecutionError,
    MediaProcessingError,
)

try:
    async with ChatAgent(model_config) as agent:
        response = await agent.chat("Hello", image_paths="invalid.jpg")

except MediaProcessingError as e:
    print(f"Media error: {e}")
    print(f"File: {e.details.get('file_path')}")
    print(f"Type: {e.details.get('media_type')}")

except APIConnectionError as e:
    print(f"Connection error: {e}")
    print(f"URL: {e.details.get('url')}")
    print(f"Status: {e.details.get('status_code')}")

except ToolExecutionError as e:
    print(f"Tool error: {e}")
    print(f"Tool: {e.details.get('tool')}")

except ChatAPIException as e:
    # æ‰€æœ‰å¼‚å¸¸çš„åŸºç±»
    print(f"API error: {e}")
    print(f"Details: {e.details}")
```

---

## ğŸ“š API å‚è€ƒ

### ChatAgent

ä¸»è¦çš„èŠå¤©ä»£ç†ç±»ã€‚

#### åˆå§‹åŒ–
```python
ChatAgent(
    model_config: ModelConfig,
    runtime_config: RuntimeConfig = None
)
```

#### æ ¸å¿ƒæ–¹æ³•

**chat()** - éæµå¼å¯¹è¯
```python
async def chat(
    text: str,
    image_paths: Union[str, List[str], None] = None,
    video_paths: Union[str, List[str], None] = None,
    add_to_history: bool = True,
    auto_execute_tools: bool = True,
    max_tool_iterations: int = 5,
    **kwargs
) -> str
```

**chat_stream()** - æµå¼å¯¹è¯
```python
async def chat_stream(
    text: str,
    image_paths: Union[str, List[str], None] = None,
    video_paths: Union[str, List[str], None] = None,
    add_to_history: bool = True,
    display_stream: bool = True,
    **kwargs
) -> AsyncGenerator[str, None]
```

**å·¥å…·ç®¡ç†**
```python
register_tool(tool: Tool, handler: Callable) -> None
clear_tools() -> None
```

**æ¶ˆæ¯ç®¡ç†**
```python
set_system_prompt(prompt: str) -> None
add_message(message: ChatMessage) -> None
clear_history(keep_system: bool = True) -> None
```

**ç»Ÿè®¡ä¿¡æ¯**
```python
get_stats() -> dict
reset_stats() -> None
```

---

### ModelConfig

æ¨¡å‹è¡Œä¸ºé…ç½®ã€‚

```python
@dataclass
class ModelConfig:
    # APIè¿æ¥
    api_base_url: str = "https://api.openai.com/v1"
    api_key: Optional[str] = None
    model: str = "gpt-4o"
    
    # é‡‡æ ·å‚æ•°
    temperature: float = 0.7
    top_p: float = 1.0
    
    # é•¿åº¦æ§åˆ¶
    max_tokens: Optional[int] = None
    max_completion_tokens: Optional[int] = None
    
    # æƒ©ç½š
    frequency_penalty: float = 0.0
    presence_penalty: float = 0.0
    
    # å…¶ä»–
    n: int = 1
    stop: Optional[Union[str, List[str]]] = None
    stream: bool = False
    logprobs: Optional[bool] = None
    top_logprobs: Optional[int] = None
    seed: Optional[int] = None
    user: Optional[str] = None
    reasoning_effort: Optional[str] = None  # "low", "medium", "high"
```

---

### RuntimeConfig

è¿è¡Œæ—¶è¡Œä¸ºé…ç½®ã€‚

```python
@dataclass
class RuntimeConfig:
    # æ—¥å¿—
    enable_logging: bool = True
    log_level: str = "INFO"
    log_http_requests: bool = False
    log_http_responses: bool = False
    
    # ç›‘æ§
    enable_debug: bool = False
    capture_token_usage: bool = True
    capture_latency: bool = True
    
    # HTTP
    timeout: int = 60
    verify_ssl: bool = True
    max_retries: int = 0
    retry_delay: float = 1.0
    
    # æµå¼
    stream_chunk_callback: Optional[Callable[[str], None]] = None
    stream_enable_progress: bool = False
    
    # è§£æ
    strict_parsing: bool = False
    truncate_long_errors: bool = True
    max_error_length: int = 500
```

---

### å¼‚å¸¸ç±»å‹

æ‰€æœ‰å¼‚å¸¸ç»§æ‰¿è‡ª `ChatAPIException`ï¼š

- **ConfigurationError** - é…ç½®éªŒè¯é”™è¯¯
- **APIConnectionError** - API è¿æ¥é”™è¯¯
- **APIResponseError** - API å“åº”è§£æé”™è¯¯
- **ToolExecutionError** - å·¥å…·æ‰§è¡Œé”™è¯¯
- **MediaProcessingError** - åª’ä½“å¤„ç†é”™è¯¯
- **ModelNotFoundError** - æ¨¡å‹æœªæ‰¾åˆ°
- **TokenLimitError** - Token é™åˆ¶è¶…å‡º

æ¯ä¸ªå¼‚å¸¸éƒ½åŒ…å« `message` å’Œ `details` å­—å…¸ã€‚

---

## ğŸ¯ ä½¿ç”¨åœºæ™¯

### åœºæ™¯1ï¼šç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

```python
# å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œç›‘æ§
runtime_config = RuntimeConfig(
    enable_logging=True,
    log_level="WARNING",
    capture_token_usage=True,
    capture_latency=True,
    max_retries=3,
    timeout=30,
)

try:
    async with ChatAgent(model_config, runtime_config) as agent:
        response = await agent.chat(user_input)
        
        # è®°å½•ç»Ÿè®¡
        stats = agent.get_stats()
        logging.info(f"Request completed, tokens: {stats['total_tokens']}")
        
except ChatAPIException as e:
    logging.error(f"Chat failed: {e}")
    # é”™è¯¯æ¢å¤é€»è¾‘
```

### åœºæ™¯2ï¼šå¼€å‘è°ƒè¯•

```python
# è¯¦ç»†æ—¥å¿—å’Œè°ƒè¯•ä¿¡æ¯
runtime_config = RuntimeConfig(
    enable_logging=True,
    log_level="DEBUG",
    log_http_requests=True,
    log_http_responses=True,
    enable_debug=True,
    verify_ssl=False,  # æœ¬åœ°æµ‹è¯•
)

async with ChatAgent(model_config, runtime_config) as agent:
    # æ‰€æœ‰è¯·æ±‚/å“åº”éƒ½ä¼šè¢«è¯¦ç»†è®°å½•
    response = await agent.chat("test")
```

### åœºæ™¯3ï¼šæµå¼å‰ç«¯å±•ç¤º

```python
from fastapi import FastAPI
from fastapi.responses import StreamingResponse

app = FastAPI()

@app.get("/chat")
async def chat_endpoint(text: str):
    async def generate():
        async with ChatAgent(model_config) as agent:
            async for chunk in agent.chat_stream(text, display_stream=False):
                yield f"data: {chunk}\n\n"
    
    return StreamingResponse(generate(), media_type="text/event-stream")
```

---

## ğŸ“ æ›´æ–°æ—¥å¿—

### v0.3.0 (å½“å‰ç‰ˆæœ¬)

**æ–°åŠŸèƒ½:**
- âœ… è§†é¢‘è¾“å…¥æ”¯æŒ
- âœ… åˆ†ç¦»çš„é…ç½®ç³»ç»Ÿï¼ˆModelConfig + RuntimeConfigï¼‰
- âœ… å®Œå–„çš„å¼‚å¸¸å¤„ç†ä½“ç³»ï¼ˆ8ç§å¼‚å¸¸ç±»å‹ï¼‰
- âœ… Tokenä½¿ç”¨ç»Ÿè®¡å’Œå»¶è¿Ÿè¿½è¸ª
- âœ… æµå¼å“åº”å®æ—¶æ˜¾ç¤º
- âœ… HTTP è¯·æ±‚/å“åº”æ—¥å¿—
- âœ… è°ƒè¯•æ¨¡å¼

**æ”¹è¿›:**
- âœ… æ›´å¥½çš„é”™è¯¯è¯Šæ–­ä¿¡æ¯
- âœ… è‡ªåŠ¨æˆªæ–­é•¿é”™è¯¯æ¶ˆæ¯
- âœ… å‘åå…¼å®¹æ—§ APIï¼ˆChatConfigï¼‰

**ç ´åæ€§å˜æ›´:**
- `ChatConfig` æ”¹åä¸º `ModelConfig`ï¼ˆä¿ç•™åˆ«åå…¼å®¹ï¼‰
- `ChatAgent.__init__` ç°åœ¨æ¥å—ä¸¤ä¸ªé…ç½®å‚æ•°

### v0.2.0

- å·¥å…·è°ƒç”¨æ”¯æŒ
- æ¨¡å‹ç®¡ç†æ¨¡å—
- å®Œæ•´çš„ OpenAI å‚æ•°æ”¯æŒ

### v0.1.0

- åŸºç¡€æ–‡æœ¬å¯¹è¯
- å›¾åƒè¾“å…¥æ”¯æŒ
- æµå¼å“åº”

---

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## ğŸ“„ è®¸å¯

MIT License

---

## ğŸ”— ç›¸å…³é“¾æ¥

- [OpenAI API æ–‡æ¡£](https://platform.openai.com/docs/api-reference)
- [httpx æ–‡æ¡£](https://www.python-httpx.org/)

---

## â“ å¸¸è§é—®é¢˜

**Q: å¦‚ä½•ä½¿ç”¨æœ¬åœ°æ¨¡å‹æœåŠ¡ï¼ˆå¦‚ Ollamaï¼‰ï¼Ÿ**

A: åªéœ€ä¿®æ”¹ `api_base_url`ï¼š
```python
model_config = ModelConfig(
    api_base_url="http://localhost:11434/v1",
    api_key="not-needed",  # Ollama ä¸éœ€è¦ key
    model="llama2",
)
```

**Q: å¦‚ä½•å¤„ç†è‡ªç­¾å SSL è¯ä¹¦ï¼Ÿ**

A: åœ¨ RuntimeConfig ä¸­å…³é—­éªŒè¯ï¼š
```python
runtime_config = RuntimeConfig(verify_ssl=False)
```

**Q: æµå¼è¾“å‡ºä¸æ˜¾ç¤ºæ€ä¹ˆåŠï¼Ÿ**

A: ç¡®ä¿å¯ç”¨äº†è¿›åº¦æ˜¾ç¤ºï¼š
```python
runtime_config = RuntimeConfig(stream_enable_progress=True)
async for chunk in agent.chat_stream(text, display_stream=True):
    pass
```

**Q: å¦‚ä½•è¿½è¸ª Token ä½¿ç”¨é‡ï¼Ÿ**

A: å¯ç”¨ç»Ÿè®¡è¿½è¸ªï¼š
```python
runtime_config = RuntimeConfig(capture_token_usage=True)
# ... ä½¿ç”¨ agent ...
stats = agent.get_stats()
print(stats)
```

**Q: æ”¯æŒå“ªäº›è§†é¢‘æ ¼å¼ï¼Ÿ**

A: æ”¯æŒå¸¸è§æ ¼å¼ï¼šmp4, webm, ogg, mov, aviã€‚è§†é¢‘ä¼šè¢« Base64 ç¼–ç åå‘é€ï¼Œæ³¨æ„å¤§å°é™åˆ¶ã€‚

**Q: å·¥å…·è°ƒç”¨å¤±è´¥æ€ä¹ˆåŠï¼Ÿ**

A: æ•è· `ToolExecutionError` æŸ¥çœ‹è¯¦æƒ…ï¼š
```python
try:
    response = await agent.chat("...", auto_execute_tools=True)
except ToolExecutionError as e:
    print(f"Tool failed: {e.details['tool']}")
    print(f"Error: {e.details['error']}")
```
